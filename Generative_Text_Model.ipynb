# Install required packages
!pip install transformers torch
# Import necessary libraries
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from google.colab import files
import os
def load_model(model_name="gpt2"):
    """
    Load pre-trained GPT-2 model and tokenizer.

    Args:
        model_name (str): Name of the model (default: "gpt2").

    Returns:
        tokenizer (GPT2Tokenizer): GPT-2 tokenizer.
        model (GPT2LMHeadModel): Pre-trained GPT-2 model.
    """
    # Load the tokenizer and model
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)

    # Move model to GPU if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    return tokenizer, model, device
def generate_text(prompt, tokenizer, model, device, max_length=100, num_return_sequences=1, temperature=1.0, top_k=50, top_p=0.95):
    """
    Generate text using a pre-trained GPT-2 model.

    Args:
        prompt (str): Input text prompt.
        tokenizer (GPT2Tokenizer): Tokenizer to encode the prompt.
        model (GPT2LMHeadModel): Pre-trained GPT-2 model.
        device (torch.device): GPU or CPU device.
        max_length (int): Maximum length of generated text.
        num_return_sequences (int): Number of sequences to generate.
        temperature (float): Sampling temperature for diversity.
        top_k (int): Limit sampling to top-k probabilities.
        top_p (float): Nucleus sampling to limit diverse outputs.

    Returns:
        list: List of generated text sequences.
    """
    # Encode the prompt
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

    # Generate text with defined parameters
    output_sequences = model.generate(
        input_ids=input_ids,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    # Decode generated text
    generated_texts = []
    for generated_sequence in output_sequences:
        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)
        generated_texts.append(text)

    return generated_texts
def save_generated_text(generated_texts, file_name="generated_text.txt"):
    """
    Save generated text to a file.

    Args:
        generated_texts (list): List of generated text sequences.
        file_name (str): Name of the output file.
    """
    with open(file_name, "w") as file:
        for i, text in enumerate(generated_texts):
            file.write(f"Generated Text {i+1}:\n{text}\n\n")
    print(f"Generated text saved as '{file_name}'")

    # Download the file in Google Colab
    files.download(file_name)
def main():
    """
    Main function to run the text generation model.
    """
    # Load pre-trained GPT-2 model and tokenizer
    print("Loading GPT-2 model...")
    tokenizer, model, device = load_model()

    # User input for prompt
    prompt = input("Enter a prompt for text generation: ")

    # Additional parameters for fine-tuning output
    max_length = int(input("Enter max length of generated text (default 100): ") or 100)
    num_return_sequences = int(input("Number of sequences to generate (default 1): ") or 1)
    temperature = float(input("Enter temperature for creativity (default 1.0): ") or 1.0)
    top_k = int(input("Enter top-k value for sampling (default 50): ") or 50)
    top_p = float(input("Enter top-p value for nucleus sampling (default 0.95): ") or 0.95)

    # Generate text based on user input
    print("\nGenerating text...")
    generated_texts = generate_text(prompt, tokenizer, model, device, max_length, num_return_sequences, temperature, top_k, top_p)

    # Display generated text
    for i, text in enumerate(generated_texts):
        print(f"\n--- Generated Text {i+1} ---\n{text}")

    # Save generated text to file
    save_generated_text(generated_texts)
if __name__ == "__main__":
    main()


